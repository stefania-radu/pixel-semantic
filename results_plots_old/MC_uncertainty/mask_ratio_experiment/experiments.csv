"input_text","MC_samples","mean_std","mean_variance","patches_count","masked_ratio","masked_count","max_span_length","cumulative_span_weights"
"After the release of ChatGPT in 2022, the number of papers published every day about Large Language Models (LLMs) has increased more than 20-fold. The number of parameters in these LLMs jumped from 340 millions in implementations such as BERT to billions of parameters in models like GPT-3 or LLaMA. A large part of these parameters come from the word-embedding layers which are used to represent a finite vocabulary of characters, sets of characters or words. Apart from increasing model complexity, a fixed vocabulary is also responsible for brittle models, which cannot deal with out-of-vocabulary inputs and cannot generalize to new languages. As a consequence, the performance in downstream tasks is also affected. ","100","0.121","0.027","256","0.9","133","6","0.2,0.4,0.6,0.8,0.9,1"
"After the release of ChatGPT in 2022, the number of papers published every day about Large Language Models (LLMs) has increased more than 20-fold. The number of parameters in these LLMs jumped from 340 millions in implementations such as BERT to billions of parameters in models like GPT-3 or LLaMA. A large part of these parameters come from the word-embedding layers which are used to represent a finite vocabulary of characters, sets of characters or words. Apart from increasing model complexity, a fixed vocabulary is also responsible for brittle models, which cannot deal with out-of-vocabulary inputs and cannot generalize to new languages. As a consequence, the performance in downstream tasks is also affected. ","100","0.214","0.085","256","0.8","133","6","0.2,0.4,0.6,0.8,0.9,1"
"After the release of ChatGPT in 2022, the number of papers published every day about Large Language Models (LLMs) has increased more than 20-fold. The number of parameters in these LLMs jumped from 340 millions in implementations such as BERT to billions of parameters in models like GPT-3 or LLaMA. A large part of these parameters come from the word-embedding layers which are used to represent a finite vocabulary of characters, sets of characters or words. Apart from increasing model complexity, a fixed vocabulary is also responsible for brittle models, which cannot deal with out-of-vocabulary inputs and cannot generalize to new languages. As a consequence, the performance in downstream tasks is also affected. ","100","0.106","0.022","256","0.7","180","6","0.2,0.4,0.6,0.8,0.9,1"
"After the release of ChatGPT in 2022, the number of papers published every day about Large Language Models (LLMs) has increased more than 20-fold. The number of parameters in these LLMs jumped from 340 millions in implementations such as BERT to billions of parameters in models like GPT-3 or LLaMA. A large part of these parameters come from the word-embedding layers which are used to represent a finite vocabulary of characters, sets of characters or words. Apart from increasing model complexity, a fixed vocabulary is also responsible for brittle models, which cannot deal with out-of-vocabulary inputs and cannot generalize to new languages. As a consequence, the performance in downstream tasks is also affected. ","100","0.106","0.022","256","0.6","154","6","0.2,0.4,0.6,0.8,0.9,1"
"After the release of ChatGPT in 2022, the number of papers published every day about Large Language Models (LLMs) has increased more than 20-fold. The number of parameters in these LLMs jumped from 340 millions in implementations such as BERT to billions of parameters in models like GPT-3 or LLaMA. A large part of these parameters come from the word-embedding layers which are used to represent a finite vocabulary of characters, sets of characters or words. Apart from increasing model complexity, a fixed vocabulary is also responsible for brittle models, which cannot deal with out-of-vocabulary inputs and cannot generalize to new languages. As a consequence, the performance in downstream tasks is also affected. ","100","0.106","0.022","256","0.5","128","6","0.2,0.4,0.6,0.8,0.9,1"
"After the release of ChatGPT in 2022, the number of papers published every day about Large Language Models (LLMs) has increased more than 20-fold. The number of parameters in these LLMs jumped from 340 millions in implementations such as BERT to billions of parameters in models like GPT-3 or LLaMA. A large part of these parameters come from the word-embedding layers which are used to represent a finite vocabulary of characters, sets of characters or words. Apart from increasing model complexity, a fixed vocabulary is also responsible for brittle models, which cannot deal with out-of-vocabulary inputs and cannot generalize to new languages. As a consequence, the performance in downstream tasks is also affected. ","100","0.1","0.017","256","0.4","103","6","0.2,0.4,0.6,0.8,0.9,1"
"After the release of ChatGPT in 2022, the number of papers published every day about Large Language Models (LLMs) has increased more than 20-fold. The number of parameters in these LLMs jumped from 340 millions in implementations such as BERT to billions of parameters in models like GPT-3 or LLaMA. A large part of these parameters come from the word-embedding layers which are used to represent a finite vocabulary of characters, sets of characters or words. Apart from increasing model complexity, a fixed vocabulary is also responsible for brittle models, which cannot deal with out-of-vocabulary inputs and cannot generalize to new languages. As a consequence, the performance in downstream tasks is also affected. ","100","0.1","0.016","256","0.3","77","6","0.2,0.4,0.6,0.8,0.9,1"
"After the release of ChatGPT in 2022, the number of papers published every day about Large Language Models (LLMs) has increased more than 20-fold. The number of parameters in these LLMs jumped from 340 millions in implementations such as BERT to billions of parameters in models like GPT-3 or LLaMA. A large part of these parameters come from the word-embedding layers which are used to represent a finite vocabulary of characters, sets of characters or words. Apart from increasing model complexity, a fixed vocabulary is also responsible for brittle models, which cannot deal with out-of-vocabulary inputs and cannot generalize to new languages. As a consequence, the performance in downstream tasks is also affected. ","100","0.103","0.016","256","0.2","52","6","0.2,0.4,0.6,0.8,0.9,1"
"After the release of ChatGPT in 2022, the number of papers published every day about Large Language Models (LLMs) has increased more than 20-fold. The number of parameters in these LLMs jumped from 340 millions in implementations such as BERT to billions of parameters in models like GPT-3 or LLaMA. A large part of these parameters come from the word-embedding layers which are used to represent a finite vocabulary of characters, sets of characters or words. Apart from increasing model complexity, a fixed vocabulary is also responsible for brittle models, which cannot deal with out-of-vocabulary inputs and cannot generalize to new languages. As a consequence, the performance in downstream tasks is also affected. ","100","0.106","0.015","256","0.1","26","6","0.2,0.4,0.6,0.8,0.9,1"