The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) 2023.01   2) StdEnv
wandb: Currently logged in as: stefania_radu. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home2/s3919609/pixel-semantic/wandb/run-20240304_091732-q3janqwe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-mountain-196
wandb: ‚≠êÔ∏è View project at https://wandb.ai/stefania_radu/pixel-semantic-scripts_visualization
wandb: üöÄ View run at https://wandb.ai/stefania_radu/pixel-semantic-scripts_visualization/runs/q3janqwe
03/04/2024 09:17:38 - INFO - pixel.data.rendering.rendering_utils - loading text renderer configuration file https://huggingface.co/Team-PIXEL/pixel-base/resolve/main/text_renderer_config.json from cache at /home2/s3919609/.cache/huggingface/transformers/892d6a02d7c441000de399de59ed70d943a81f7b0f536523b4af1111677a8508.e332b34c9c05756dd4aa51d8fa33461dbd79604752296d185f03f8004db30700
03/04/2024 09:17:38 - INFO - pixel.data.rendering.rendering_utils - loading font file https://huggingface.co/Team-PIXEL/pixel-base/resolve/main/GoNotoCurrent.ttf from cache at /home2/s3919609/.cache/huggingface/transformers/49e6dc219d1a1a1c9236acaf05a48b542002016a6dc877ee72baab085a84257b.3f28e7f4b38e1efe1b6da4a3732404c19d4c6a614ff32dce90a251e293d4ce58
03/04/2024 09:17:38 - INFO - pixel.data.rendering.pygame_renderer - Loading font from /home2/s3919609/.cache/huggingface/transformers/49e6dc219d1a1a1c9236acaf05a48b542002016a6dc877ee72baab085a84257b.3f28e7f4b38e1efe1b6da4a3732404c19d4c6a614ff32dce90a251e293d4ce58
03/04/2024 09:17:38 - INFO - pixel.data.rendering.rendering_utils - Text renderer PyGameTextRenderer {
  "background_color": "white",
  "dpi": 120,
  "font_color": "black",
  "font_file": "49e6dc219d1a1a1c9236acaf05a48b542002016a6dc877ee72baab085a84257b.3f28e7f4b38e1efe1b6da4a3732404c19d4c6a614ff32dce90a251e293d4ce58",
  "font_size": 8,
  "max_seq_length": 256,
  "pad_size": 3,
  "pixels_per_patch": 16,
  "text_renderer_type": "PyGameTextRenderer"
}

03/04/2024 09:17:41 - INFO - pixel.utils.modeling - Truncating position embeddings to 256
03/04/2024 09:17:41 - INFO - pixel.utils.modeling - Truncating decoder position embeddings to 256
03/04/2024 09:17:41 - INFO - __main__ - Running PIXEL masked autoencoding with pixel reconstruction
03/04/2024 09:17:41 - INFO - __main__ - Applying span masking with "max_span_length = 6" , "cumulative_span_weights = [0.2, 0.4, 0.6, 0.8, 0.9, 1.0]"  and "spacing = 1"
03/04/2024 09:17:41 - INFO - __main__ - Masked count: 128, ratio = 0.5000
03/04/2024 09:17:41 - INFO - __main__ - Monte Carlo samples: 100
03/04/2024 09:17:41 - INFO - __main__ - Training mode: True
all_attention (samples, layers, batch_size, num_heads, sequence_length, sequence_length): torch.Size([100, 12, 12, 129, 129])
all_attention after mean: torch.Size([12, 12, 129, 129])
03/04/2024 09:19:31 - INFO - __main__ - std_predictions shape: torch.Size([3, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 129, 129])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 129, 129])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 129, 129])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 129, 129])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 129, 129])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 129, 129])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 129, 129])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 129, 129])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 129, 129])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 129, 129])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 129, 129])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 129, 129])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
all_layers_attentions: torch.Size([12, 12, 256, 256])
attention_grid: torch.Size([3, 3098, 3098])
attention_grid 0 : tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 1.0000,  ..., 0.8308, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])
attention_grid 1 : tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 1.0000,  ..., 0.8308, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])
Are the channels different? False
03/04/2024 09:19:39 - INFO - __main__ - Mean variance for whole image: 0.022
03/04/2024 09:19:39 - INFO - __main__ - Mean std for whole image: 0.106
03/04/2024 09:19:40 - INFO - __main__ - mean_std shape: (3, 256, 256)
03/04/2024 09:19:40 - INFO - __main__ - std_predictions shape: torch.Size([3, 256, 256])
mean_predictions: tensor([[[ 0.5564,  0.5587,  0.5583,  ...,  0.5460,  0.5474,  0.5487],
         [ 0.5774,  0.5606,  0.5482,  ...,  0.4826,  0.3174,  0.4844],
         [ 0.5406,  0.5312,  0.4181,  ...,  0.2833, -0.4395,  0.2533],
         ...,
         [ 0.5226,  0.2544,  0.6143,  ...,  0.6148,  0.6632,  0.6157],
         [ 0.4602,  0.5756,  0.6201,  ...,  0.3030,  0.4866,  0.6734],
         [ 0.4589,  0.5334,  0.5714,  ...,  0.7895,  0.7201,  0.6303]],

        [[ 0.5564,  0.5587,  0.5583,  ...,  0.5460,  0.5474,  0.5487],
         [ 0.5774,  0.5606,  0.5482,  ...,  0.4826,  0.3174,  0.4844],
         [ 0.5407,  0.5312,  0.4181,  ...,  0.2833, -0.4395,  0.2533],
         ...,
         [ 0.5226,  0.2544,  0.6143,  ...,  0.6148,  0.6632,  0.6158],
         [ 0.4602,  0.5756,  0.6201,  ...,  0.3030,  0.4865,  0.6734],
         [ 0.4589,  0.5333,  0.5714,  ...,  0.7895,  0.7201,  0.6303]],

        [[ 0.5564,  0.5587,  0.5583,  ...,  0.5460,  0.5474,  0.5487],
         [ 0.5774,  0.5605,  0.5482,  ...,  0.4826,  0.3174,  0.4845],
         [ 0.5407,  0.5312,  0.4181,  ...,  0.2833, -0.4395,  0.2533],
         ...,
         [ 0.5226,  0.2544,  0.6144,  ...,  0.6148,  0.6632,  0.6157],
         [ 0.4602,  0.5756,  0.6201,  ...,  0.3030,  0.4866,  0.6734],
         [ 0.4589,  0.5334,  0.5715,  ...,  0.7895,  0.7201,  0.6303]]])
std_predictions_per_patch: tensor([18.0507, 18.1238, 18.1105, 18.1026, 18.0868, 18.0309, 18.0945, 18.0901,
        18.0924, 18.1186, 18.1105, 18.1239, 18.0972, 18.0127, 18.0791, 18.0724,
        12.9503, 12.9311, 12.9359, 12.9584, 12.9305, 12.9144, 12.9264, 12.9237,
        12.9495, 12.9321, 12.9475, 12.9623, 12.9583, 12.9440, 12.9391, 12.9521,
         6.6768,  6.6908,  6.6867,  6.6828,  6.6880,  6.6785,  6.6830,  6.6818,
         6.6915,  6.6947,  6.6843,  6.6806,  6.6629,  6.6660,  6.6788,  6.6852,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         3.3764,  3.3757,  3.3784,  3.3836,  3.3785,  3.3807,  3.3803,  3.3774,
         3.3768,  3.3745,  3.3741,  3.3787,  3.3734,  3.3763,  3.3753,  3.3794,
         5.4476,  5.4391,  5.4358,  5.4363,  5.4339,  5.4397,  5.4388,  5.4408,
         5.4525,  5.4329,  5.4362,  5.4380,  5.4407,  5.4430,  5.4373,  5.4416,
        24.3210, 24.2980, 24.3000, 24.2928, 24.2986, 24.2988, 24.2830, 24.2824,
        24.2787, 24.2795, 24.2862, 24.2702, 24.2974, 24.3001, 24.2716, 24.2858,
        17.3134, 17.3080, 17.3621, 17.3595, 17.3142, 17.3478, 17.3715, 17.3927,
        17.3409, 17.3070, 17.3169, 17.3255, 17.3650, 17.3334, 17.3219, 17.3544,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])
std_reconstruction_per_patch: tensor([18.0507, 18.1238, 18.1105, 18.1026, 18.0868, 18.0309, 18.0945, 18.0901,
        18.0924, 18.1186, 18.1105, 18.1239, 18.0972, 18.0127, 18.0791, 18.0724,
        12.9503, 12.9311, 12.9359, 12.9584, 12.9305, 12.9144, 12.9264, 12.9237,
        12.9495, 12.9321, 12.9475, 12.9623, 12.9583, 12.9440, 12.9391, 12.9521,
         6.6768,  6.6908,  6.6867,  6.6828,  6.6880,  6.6785,  6.6830,  6.6818,
         6.6915,  6.6947,  6.6843,  6.6806,  6.6629,  6.6660,  6.6788,  6.6852,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         3.3764,  3.3757,  3.3784,  3.3836,  3.3785,  3.3807,  3.3803,  3.3774,
         3.3768,  3.3745,  3.3741,  3.3787,  3.3734,  3.3763,  3.3753,  3.3794,
         5.4476,  5.4391,  5.4358,  5.4363,  5.4339,  5.4397,  5.4388,  5.4408,
         5.4525,  5.4329,  5.4362,  5.4380,  5.4407,  5.4430,  5.4373,  5.4416,
        24.3210, 24.2980, 24.3000, 24.2928, 24.2986, 24.2988, 24.2830, 24.2824,
        24.2787, 24.2795, 24.2862, 24.2702, 24.2974, 24.3001, 24.2716, 24.2858,
        17.3134, 17.3080, 17.3621, 17.3595, 17.3142, 17.3478, 17.3715, 17.3927,
        17.3409, 17.3070, 17.3169, 17.3255, 17.3650, 17.3334, 17.3219, 17.3544,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])
03/04/2024 09:19:42 - INFO - __main__ - torch.Size([3, 256, 256])
03/04/2024 09:19:42 - INFO - __main__ - torch.Size([3, 256, 256])
wandb: Waiting for W&B process to finish... (success).
wandb: - 1.789 MB of 1.790 MB uploaded (0.000 MB deduped)wandb: \ 1.789 MB of 1.790 MB uploaded (0.000 MB deduped)wandb: | 1.789 MB of 1.790 MB uploaded (0.000 MB deduped)wandb: / 1.789 MB of 1.790 MB uploaded (0.000 MB deduped)wandb: - 1.789 MB of 1.790 MB uploaded (0.000 MB deduped)wandb: \ 1.789 MB of 1.790 MB uploaded (0.000 MB deduped)wandb: | 1.789 MB of 1.790 MB uploaded (0.000 MB deduped)wandb: / 1.789 MB of 1.790 MB uploaded (0.000 MB deduped)wandb: - 1.789 MB of 1.790 MB uploaded (0.000 MB deduped)wandb: \ 1.789 MB of 1.790 MB uploaded (0.000 MB deduped)wandb: | 1.789 MB of 1.790 MB uploaded (0.000 MB deduped)wandb: / 1.789 MB of 1.790 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:      mean_std_value ‚ñÅ
wandb: mean_variance_value ‚ñÅ
wandb: 
wandb: Run summary:
wandb:      mean_std_value 0.106
wandb: mean_variance_value 0.022
wandb: 
wandb: üöÄ View run swift-mountain-196 at: https://wandb.ai/stefania_radu/pixel-semantic-scripts_visualization/runs/q3janqwe
wandb: Ô∏è‚ö° View job at https://wandb.ai/stefania_radu/pixel-semantic-scripts_visualization/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExOTExMzkwMw==/version_details/v46
wandb: Synced 6 W&B file(s), 15 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240304_091732-q3janqwe/logs

###############################################################################
H√°br√≥k Cluster
Job 7485384 for user s3919609
Finished at: Mon Mar  4 09:19:59 CET 2024

Job details:
============

Job ID              : 7485384
Name                : experiments_pixel_uncertainty_m0.5
User                : s3919609
Partition           : regularshort
Nodes               : node103
Number of Nodes     : 1
Cores               : 1
Number of Tasks     : 1
State               : RUNNING
Submit              : 2024-03-04T09:15:07
Start               : 2024-03-04T09:17:13
End                 : --
Reserved walltime   : 01:00:00
Used walltime       : 00:02:46
Used CPU time       : --
% User (Computation): --
% System (I/O)      : --
Mem reserved        : 10G
Max Mem (Node/step) : 0.00  (Node unknown, N/A)
Full Max Mem usage  : 0.00  (Until last completed step)
Total Disk Read     : 0.00  (Until last completed step)
Total Disk Write    : 0.00  (Until last completed step)

Acknowledgements:
=================

Please see this page for information about acknowledging H√°br√≥k in your publications:

https://wiki.hpc.rug.nl/habrok/introduction/scientific_output

################################################################################
