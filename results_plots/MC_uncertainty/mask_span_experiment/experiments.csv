"input_text","MC_samples","mean_std","mean_variance","patches_count","masked_ratio","masked_count","max_span_length","cumulative_span_weights"
"After the release of ChatGPT in 2022, the number of papers published every day about Large Language Models (LLMs) has increased more than 20-fold. The number of parameters in these LLMs jumped from 340 millions in implementations such as BERT to billions of parameters in models like GPT-3 or LLaMA. A large part of these parameters come from the word-embedding layers which are used to represent a finite vocabulary of characters, sets of characters or words. Apart from increasing model complexity, a fixed vocabulary is also responsible for brittle models, which cannot deal with out-of-vocabulary inputs and cannot generalize to new languages. As a consequence, the performance in downstream tasks is also affected. ","100","0.1","0.016","256","0.25","64","1","1"
"After the release of ChatGPT in 2022, the number of papers published every day about Large Language Models (LLMs) has increased more than 20-fold. The number of parameters in these LLMs jumped from 340 millions in implementations such as BERT to billions of parameters in models like GPT-3 or LLaMA. A large part of these parameters come from the word-embedding layers which are used to represent a finite vocabulary of characters, sets of characters or words. Apart from increasing model complexity, a fixed vocabulary is also responsible for brittle models, which cannot deal with out-of-vocabulary inputs and cannot generalize to new languages. As a consequence, the performance in downstream tasks is also affected. ","100","0.1","0.016","256","0.25","64","2","0,1"
"After the release of ChatGPT in 2022, the number of papers published every day about Large Language Models (LLMs) has increased more than 20-fold. The number of parameters in these LLMs jumped from 340 millions in implementations such as BERT to billions of parameters in models like GPT-3 or LLaMA. A large part of these parameters come from the word-embedding layers which are used to represent a finite vocabulary of characters, sets of characters or words. Apart from increasing model complexity, a fixed vocabulary is also responsible for brittle models, which cannot deal with out-of-vocabulary inputs and cannot generalize to new languages. As a consequence, the performance in downstream tasks is also affected. ","100","0.113","0.02","256","0.25","63","3","0,0,1"
"After the release of ChatGPT in 2022, the number of papers published every day about Large Language Models (LLMs) has increased more than 20-fold. The number of parameters in these LLMs jumped from 340 millions in implementations such as BERT to billions of parameters in models like GPT-3 or LLaMA. A large part of these parameters come from the word-embedding layers which are used to represent a finite vocabulary of characters, sets of characters or words. Apart from increasing model complexity, a fixed vocabulary is also responsible for brittle models, which cannot deal with out-of-vocabulary inputs and cannot generalize to new languages. As a consequence, the performance in downstream tasks is also affected. ","100","0.1","0.015","256","0.25","64","4","0,0,0,1"
"After the release of ChatGPT in 2022, the number of papers published every day about Large Language Models (LLMs) has increased more than 20-fold. The number of parameters in these LLMs jumped from 340 millions in implementations such as BERT to billions of parameters in models like GPT-3 or LLaMA. A large part of these parameters come from the word-embedding layers which are used to represent a finite vocabulary of characters, sets of characters or words. Apart from increasing model complexity, a fixed vocabulary is also responsible for brittle models, which cannot deal with out-of-vocabulary inputs and cannot generalize to new languages. As a consequence, the performance in downstream tasks is also affected. ","100","0.144","0.034","256","0.25","60","5","0,0,0,0,1"
"After the release of ChatGPT in 2022, the number of papers published every day about Large Language Models (LLMs) has increased more than 20-fold. The number of parameters in these LLMs jumped from 340 millions in implementations such as BERT to billions of parameters in models like GPT-3 or LLaMA. A large part of these parameters come from the word-embedding layers which are used to represent a finite vocabulary of characters, sets of characters or words. Apart from increasing model complexity, a fixed vocabulary is also responsible for brittle models, which cannot deal with out-of-vocabulary inputs and cannot generalize to new languages. As a consequence, the performance in downstream tasks is also affected. ","100","0.142","0.033","256","0.25","60","6","0,0,0,0,0,1"

