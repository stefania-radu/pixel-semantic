The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) 2023.01   2) StdEnv
wandb: Currently logged in as: stefania_radu. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home2/s3919609/pixel-semantic/wandb/run-20240304_092406-l2cfzkul
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-morning-197
wandb: ‚≠êÔ∏è View project at https://wandb.ai/stefania_radu/pixel-semantic-scripts_visualization
wandb: üöÄ View run at https://wandb.ai/stefania_radu/pixel-semantic-scripts_visualization/runs/l2cfzkul
03/04/2024 09:24:11 - INFO - pixel.data.rendering.rendering_utils - loading text renderer configuration file https://huggingface.co/Team-PIXEL/pixel-base/resolve/main/text_renderer_config.json from cache at /home2/s3919609/.cache/huggingface/transformers/892d6a02d7c441000de399de59ed70d943a81f7b0f536523b4af1111677a8508.e332b34c9c05756dd4aa51d8fa33461dbd79604752296d185f03f8004db30700
03/04/2024 09:24:12 - INFO - pixel.data.rendering.rendering_utils - loading font file https://huggingface.co/Team-PIXEL/pixel-base/resolve/main/GoNotoCurrent.ttf from cache at /home2/s3919609/.cache/huggingface/transformers/49e6dc219d1a1a1c9236acaf05a48b542002016a6dc877ee72baab085a84257b.3f28e7f4b38e1efe1b6da4a3732404c19d4c6a614ff32dce90a251e293d4ce58
03/04/2024 09:24:12 - INFO - pixel.data.rendering.pygame_renderer - Loading font from /home2/s3919609/.cache/huggingface/transformers/49e6dc219d1a1a1c9236acaf05a48b542002016a6dc877ee72baab085a84257b.3f28e7f4b38e1efe1b6da4a3732404c19d4c6a614ff32dce90a251e293d4ce58
03/04/2024 09:24:12 - INFO - pixel.data.rendering.rendering_utils - Text renderer PyGameTextRenderer {
  "background_color": "white",
  "dpi": 120,
  "font_color": "black",
  "font_file": "49e6dc219d1a1a1c9236acaf05a48b542002016a6dc877ee72baab085a84257b.3f28e7f4b38e1efe1b6da4a3732404c19d4c6a614ff32dce90a251e293d4ce58",
  "font_size": 8,
  "max_seq_length": 256,
  "pad_size": 3,
  "pixels_per_patch": 16,
  "text_renderer_type": "PyGameTextRenderer"
}

03/04/2024 09:24:14 - INFO - pixel.utils.modeling - Truncating position embeddings to 256
03/04/2024 09:24:14 - INFO - pixel.utils.modeling - Truncating decoder position embeddings to 256
03/04/2024 09:24:14 - INFO - __main__ - Running PIXEL masked autoencoding with pixel reconstruction
03/04/2024 09:24:14 - INFO - __main__ - Applying span masking with "max_span_length = 6" , "cumulative_span_weights = [0.2, 0.4, 0.6, 0.8, 0.9, 1.0]"  and "spacing = 1"
03/04/2024 09:24:14 - INFO - __main__ - Masked count: 154, ratio = 0.6016
03/04/2024 09:24:14 - INFO - __main__ - Monte Carlo samples: 100
03/04/2024 09:24:14 - INFO - __main__ - Training mode: True
all_attention (samples, layers, batch_size, num_heads, sequence_length, sequence_length): torch.Size([100, 12, 12, 103, 103])
all_attention after mean: torch.Size([12, 12, 103, 103])
03/04/2024 09:25:40 - INFO - __main__ - std_predictions shape: torch.Size([3, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 103, 103])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 103, 103])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 103, 103])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 103, 103])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 103, 103])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 103, 103])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 103, 103])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 103, 103])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 103, 103])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 103, 103])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 103, 103])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 103, 103])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
all_layers_attentions: torch.Size([12, 12, 256, 256])
attention_grid: torch.Size([3, 3098, 3098])
attention_grid 0 : tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0232, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 1.0000,  ..., 0.7798, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])
attention_grid 1 : tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0232, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 1.0000,  ..., 0.7798, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])
Are the channels different? False
03/04/2024 09:25:47 - INFO - __main__ - Mean variance for whole image: 0.022
03/04/2024 09:25:47 - INFO - __main__ - Mean std for whole image: 0.106
03/04/2024 09:25:47 - INFO - __main__ - mean_std shape: (3, 256, 256)
03/04/2024 09:25:47 - INFO - __main__ - std_predictions shape: torch.Size([3, 256, 256])
mean_predictions: tensor([[[ 0.5584,  0.5604,  0.5602,  ...,  0.5445,  0.5457,  0.5471],
         [ 0.5835,  0.5572,  0.5477,  ...,  0.4754,  0.3224,  0.4800],
         [ 0.5541,  0.5276,  0.3011,  ...,  0.3374, -0.2657,  0.2711],
         ...,
         [ 0.5839,  0.4903,  0.5954,  ...,  0.6102,  0.6636,  0.6008],
         [ 0.5974,  0.6153,  0.6113,  ...,  0.3036,  0.4704,  0.6796],
         [ 0.5791,  0.6166,  0.6023,  ...,  0.7814,  0.7290,  0.6369]],

        [[ 0.5584,  0.5604,  0.5602,  ...,  0.5445,  0.5457,  0.5471],
         [ 0.5835,  0.5572,  0.5477,  ...,  0.4754,  0.3224,  0.4800],
         [ 0.5541,  0.5276,  0.3011,  ...,  0.3374, -0.2656,  0.2711],
         ...,
         [ 0.5839,  0.4903,  0.5954,  ...,  0.6102,  0.6636,  0.6008],
         [ 0.5974,  0.6153,  0.6113,  ...,  0.3036,  0.4704,  0.6795],
         [ 0.5791,  0.6166,  0.6023,  ...,  0.7815,  0.7290,  0.6369]],

        [[ 0.5584,  0.5604,  0.5602,  ...,  0.5445,  0.5457,  0.5471],
         [ 0.5835,  0.5572,  0.5477,  ...,  0.4754,  0.3224,  0.4801],
         [ 0.5541,  0.5276,  0.3011,  ...,  0.3374, -0.2657,  0.2711],
         ...,
         [ 0.5839,  0.4903,  0.5954,  ...,  0.6103,  0.6636,  0.6008],
         [ 0.5974,  0.6153,  0.6114,  ...,  0.3036,  0.4704,  0.6796],
         [ 0.5791,  0.6166,  0.6023,  ...,  0.7815,  0.7290,  0.6369]]])
std_predictions_per_patch: tensor([22.1744, 22.2560, 22.2469, 22.2264, 22.2135, 22.1516, 22.2162, 22.2205,
        22.2168, 22.2418, 22.2448, 22.2518, 22.2295, 22.1390, 22.1986, 22.2015,
        14.1903, 14.1697, 14.1726, 14.1993, 14.1698, 14.1514, 14.1615, 14.1604,
        14.1850, 14.1682, 14.1853, 14.1997, 14.1978, 14.1817, 14.1792, 14.1912,
         6.8078,  6.8197,  6.8153,  6.8134,  6.8165,  6.8070,  6.8131,  6.8125,
         6.8204,  6.8240,  6.8132,  6.8097,  6.7910,  6.7934,  6.8103,  6.8166,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         4.0151,  4.0138,  4.0177,  4.0240,  4.0176,  4.0200,  4.0197,  4.0166,
         4.0149,  4.0119,  4.0126,  4.0180,  4.0115,  4.0154,  4.0141,  4.0184,
         6.8792,  6.8683,  6.8643,  6.8654,  6.8636,  6.8704,  6.8678,  6.8709,
         6.8846,  6.8599,  6.8659,  6.8674,  6.8712,  6.8744,  6.8661,  6.8721,
        19.1233, 19.1117, 19.1105, 19.1058, 19.1143, 19.1093, 19.1006, 19.1023,
        19.0950, 19.0975, 19.1034, 19.0931, 19.1124, 19.1101, 19.0948, 19.1060,
        15.9751, 15.9685, 16.0139, 16.0108, 15.9741, 16.0016, 16.0206, 16.0438,
        16.0015, 15.9668, 15.9745, 15.9838, 16.0221, 15.9906, 15.9780, 16.0117,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         9.9968,  9.9936, 10.0078,  9.9939,  9.9915, 10.0042,  9.9554,  9.9441,
         9.9544,  9.9548,  9.9900,  9.9662,  9.9662,  9.9902,  9.9579,  9.9862,
        11.0687, 11.1173, 11.0802, 11.0573, 11.0963, 11.0649, 11.0708, 11.0999,
        11.0808, 11.0993, 11.0726, 11.0635, 11.0995, 11.0547, 11.0649, 11.1094,
         6.0005,  6.0117,  6.0053,  6.0132,  6.0090,  6.0110,  6.0123,  6.0056,
         5.9987,  6.0201,  6.0059,  6.0041,  6.0198,  6.0117,  6.0228,  6.0382,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])
std_reconstruction_per_patch: tensor([22.1744, 22.2560, 22.2469, 22.2264, 22.2135, 22.1516, 22.2162, 22.2205,
        22.2168, 22.2418, 22.2448, 22.2518, 22.2295, 22.1390, 22.1986, 22.2015,
        14.1903, 14.1697, 14.1726, 14.1993, 14.1698, 14.1514, 14.1615, 14.1604,
        14.1850, 14.1682, 14.1853, 14.1997, 14.1978, 14.1817, 14.1792, 14.1912,
         6.8078,  6.8197,  6.8153,  6.8134,  6.8165,  6.8070,  6.8131,  6.8125,
         6.8204,  6.8240,  6.8132,  6.8097,  6.7910,  6.7934,  6.8103,  6.8166,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         4.0151,  4.0138,  4.0177,  4.0240,  4.0176,  4.0200,  4.0197,  4.0166,
         4.0149,  4.0119,  4.0126,  4.0180,  4.0115,  4.0154,  4.0141,  4.0184,
         6.8792,  6.8683,  6.8643,  6.8654,  6.8636,  6.8704,  6.8678,  6.8709,
         6.8846,  6.8599,  6.8659,  6.8674,  6.8712,  6.8744,  6.8661,  6.8721,
        19.1233, 19.1117, 19.1105, 19.1058, 19.1143, 19.1093, 19.1006, 19.1023,
        19.0950, 19.0975, 19.1034, 19.0931, 19.1124, 19.1101, 19.0948, 19.1060,
        15.9751, 15.9685, 16.0139, 16.0108, 15.9741, 16.0016, 16.0206, 16.0438,
        16.0015, 15.9668, 15.9745, 15.9838, 16.0221, 15.9906, 15.9780, 16.0117,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         9.9968,  9.9936, 10.0078,  9.9939,  9.9915, 10.0042,  9.9554,  9.9441,
         9.9544,  9.9548,  9.9900,  9.9662,  9.9662,  9.9902,  9.9579,  9.9862,
        11.0687, 11.1173, 11.0802, 11.0573, 11.0963, 11.0649, 11.0708, 11.0999,
        11.0808, 11.0993, 11.0726, 11.0635, 11.0995, 11.0547, 11.0649, 11.1094,
         6.0005,  6.0117,  6.0053,  6.0132,  6.0090,  6.0110,  6.0123,  6.0056,
         5.9987,  6.0201,  6.0059,  6.0041,  6.0198,  6.0117,  6.0228,  6.0382,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])
03/04/2024 09:25:53 - INFO - __main__ - torch.Size([3, 256, 256])
03/04/2024 09:25:53 - INFO - __main__ - torch.Size([3, 256, 256])
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:      mean_std_value ‚ñÅ
wandb: mean_variance_value ‚ñÅ
wandb: 
wandb: Run summary:
wandb:      mean_std_value 0.106
wandb: mean_variance_value 0.022
wandb: 
wandb: üöÄ View run dainty-morning-197 at: https://wandb.ai/stefania_radu/pixel-semantic-scripts_visualization/runs/l2cfzkul
wandb: Ô∏è‚ö° View job at https://wandb.ai/stefania_radu/pixel-semantic-scripts_visualization/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExOTExMzkwMw==/version_details/v47
wandb: Synced 6 W&B file(s), 15 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240304_092406-l2cfzkul/logs

###############################################################################
H√°br√≥k Cluster
Job 7485409 for user s3919609
Finished at: Mon Mar  4 09:26:03 CET 2024

Job details:
============

Job ID              : 7485409
Name                : experiments_pixel_uncertainty_m0.6
User                : s3919609
Partition           : regularshort
Nodes               : node103
Number of Nodes     : 1
Cores               : 1
Number of Tasks     : 1
State               : COMPLETED
Submit              : 2024-03-04T09:22:09
Start               : 2024-03-04T09:23:49
End                 : 2024-03-04T09:26:03
Reserved walltime   : 01:00:00
Used walltime       : 00:02:14
Used CPU time       : 00:01:54 (efficiency: 85.27%)
% User (Computation): 93.17%
% System (I/O)      :  6.83%
Mem reserved        : 10G
Max Mem (Node/step) : 2.35G (node103, per node)
Full Max Mem usage  : 2.35G
Total Disk Read     : 785.45M
Total Disk Write    : 57.58M

Acknowledgements:
=================

Please see this page for information about acknowledging H√°br√≥k in your publications:

https://wiki.hpc.rug.nl/habrok/introduction/scientific_output

################################################################################
