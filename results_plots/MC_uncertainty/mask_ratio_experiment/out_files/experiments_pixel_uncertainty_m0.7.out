The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) 2023.01   2) StdEnv
wandb: Currently logged in as: stefania_radu. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home2/s3919609/pixel-semantic/wandb/run-20240304_092830-rqc1n7p3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pious-feather-198
wandb: ‚≠êÔ∏è View project at https://wandb.ai/stefania_radu/pixel-semantic-scripts_visualization
wandb: üöÄ View run at https://wandb.ai/stefania_radu/pixel-semantic-scripts_visualization/runs/rqc1n7p3
03/04/2024 09:28:36 - INFO - pixel.data.rendering.rendering_utils - loading text renderer configuration file https://huggingface.co/Team-PIXEL/pixel-base/resolve/main/text_renderer_config.json from cache at /home2/s3919609/.cache/huggingface/transformers/892d6a02d7c441000de399de59ed70d943a81f7b0f536523b4af1111677a8508.e332b34c9c05756dd4aa51d8fa33461dbd79604752296d185f03f8004db30700
03/04/2024 09:28:36 - INFO - pixel.data.rendering.rendering_utils - loading font file https://huggingface.co/Team-PIXEL/pixel-base/resolve/main/GoNotoCurrent.ttf from cache at /home2/s3919609/.cache/huggingface/transformers/49e6dc219d1a1a1c9236acaf05a48b542002016a6dc877ee72baab085a84257b.3f28e7f4b38e1efe1b6da4a3732404c19d4c6a614ff32dce90a251e293d4ce58
03/04/2024 09:28:36 - INFO - pixel.data.rendering.pygame_renderer - Loading font from /home2/s3919609/.cache/huggingface/transformers/49e6dc219d1a1a1c9236acaf05a48b542002016a6dc877ee72baab085a84257b.3f28e7f4b38e1efe1b6da4a3732404c19d4c6a614ff32dce90a251e293d4ce58
03/04/2024 09:28:36 - INFO - pixel.data.rendering.rendering_utils - Text renderer PyGameTextRenderer {
  "background_color": "white",
  "dpi": 120,
  "font_color": "black",
  "font_file": "49e6dc219d1a1a1c9236acaf05a48b542002016a6dc877ee72baab085a84257b.3f28e7f4b38e1efe1b6da4a3732404c19d4c6a614ff32dce90a251e293d4ce58",
  "font_size": 8,
  "max_seq_length": 256,
  "pad_size": 3,
  "pixels_per_patch": 16,
  "text_renderer_type": "PyGameTextRenderer"
}

03/04/2024 09:28:38 - INFO - pixel.utils.modeling - Truncating position embeddings to 256
03/04/2024 09:28:38 - INFO - pixel.utils.modeling - Truncating decoder position embeddings to 256
03/04/2024 09:28:38 - INFO - __main__ - Running PIXEL masked autoencoding with pixel reconstruction
03/04/2024 09:28:38 - INFO - __main__ - Applying span masking with "max_span_length = 6" , "cumulative_span_weights = [0.2, 0.4, 0.6, 0.8, 0.9, 1.0]"  and "spacing = 1"
03/04/2024 09:28:38 - INFO - __main__ - Masked count: 180, ratio = 0.7031
03/04/2024 09:28:38 - INFO - __main__ - Monte Carlo samples: 100
03/04/2024 09:28:38 - INFO - __main__ - Training mode: True
all_attention (samples, layers, batch_size, num_heads, sequence_length, sequence_length): torch.Size([100, 12, 12, 77, 77])
all_attention after mean: torch.Size([12, 12, 77, 77])
03/04/2024 09:29:52 - INFO - __main__ - std_predictions shape: torch.Size([3, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 77, 77])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 77, 77])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 77, 77])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 77, 77])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 77, 77])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 77, 77])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 77, 77])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 77, 77])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 77, 77])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 77, 77])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 77, 77])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 77, 77])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
all_layers_attentions: torch.Size([12, 12, 256, 256])
attention_grid: torch.Size([3, 3098, 3098])
attention_grid 0 : tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0306, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 1.0000,  ..., 0.8428, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])
attention_grid 1 : tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0306, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 1.0000,  ..., 0.8428, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])
Are the channels different? False
03/04/2024 09:29:59 - INFO - __main__ - Mean variance for whole image: 0.022
03/04/2024 09:29:59 - INFO - __main__ - Mean std for whole image: 0.106
03/04/2024 09:29:59 - INFO - __main__ - mean_std shape: (3, 256, 256)
03/04/2024 09:29:59 - INFO - __main__ - std_predictions shape: torch.Size([3, 256, 256])
mean_predictions: tensor([[[ 0.5344,  0.5351,  0.5350,  ...,  0.5526,  0.5540,  0.5554],
         [ 0.5256,  0.5327,  0.5200,  ...,  0.4719,  0.3170,  0.4547],
         [ 0.5363,  0.4969, -0.6472,  ...,  0.3052, -0.2185,  0.2945],
         ...,
         [ 0.6062,  0.5655,  0.5940,  ...,  0.6007,  0.6693,  0.5960],
         [ 0.6407,  0.6198,  0.6015,  ...,  0.3270,  0.4837,  0.6708],
         [ 0.6155,  0.6393,  0.6065,  ...,  0.7555,  0.7172,  0.6349]],

        [[ 0.5344,  0.5351,  0.5350,  ...,  0.5526,  0.5540,  0.5554],
         [ 0.5256,  0.5327,  0.5200,  ...,  0.4719,  0.3170,  0.4547],
         [ 0.5363,  0.4969, -0.6472,  ...,  0.3052, -0.2184,  0.2945],
         ...,
         [ 0.6062,  0.5654,  0.5940,  ...,  0.6006,  0.6693,  0.5961],
         [ 0.6407,  0.6198,  0.6015,  ...,  0.3270,  0.4837,  0.6708],
         [ 0.6155,  0.6392,  0.6065,  ...,  0.7555,  0.7172,  0.6349]],

        [[ 0.5344,  0.5351,  0.5350,  ...,  0.5526,  0.5540,  0.5554],
         [ 0.5256,  0.5327,  0.5200,  ...,  0.4719,  0.3170,  0.4547],
         [ 0.5363,  0.4969, -0.6472,  ...,  0.3052, -0.2184,  0.2945],
         ...,
         [ 0.6062,  0.5654,  0.5940,  ...,  0.6007,  0.6693,  0.5961],
         [ 0.6407,  0.6198,  0.6015,  ...,  0.3270,  0.4837,  0.6708],
         [ 0.6155,  0.6393,  0.6065,  ...,  0.7555,  0.7172,  0.6349]]])
std_predictions_per_patch: tensor([26.9938, 27.0290, 27.0266, 27.0009, 27.0090, 26.9919, 27.0067, 26.9976,
        27.0009, 27.0104, 27.0225, 27.0193, 27.0111, 26.9966, 26.9964, 26.9839,
        28.0171, 28.0094, 28.0022, 28.0172, 28.0114, 27.9783, 27.9820, 27.9940,
        28.0081, 27.9963, 28.0012, 28.0153, 28.0233, 28.0005, 28.0042, 28.0200,
        28.7452, 28.7560, 28.7502, 28.7484, 28.7603, 28.7386, 28.7562, 28.7597,
        28.7456, 28.7348, 28.7395, 28.7480, 28.7356, 28.7371, 28.7505, 28.7557,
        17.5249, 17.5373, 17.5418, 17.5371, 17.5383, 17.5264, 17.5477, 17.5309,
        17.5185, 17.5167, 17.5299, 17.5131, 17.5093, 17.5224, 17.5211, 17.5265,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
        10.0087, 10.0059, 10.0142, 10.0244, 10.0108, 10.0212, 10.0230, 10.0130,
        10.0089, 10.0093, 10.0085, 10.0182, 10.0069, 10.0147, 10.0167, 10.0169,
        19.7662, 19.7463, 19.7408, 19.7463, 19.7457, 19.7583, 19.7552, 19.7618,
        19.7600, 19.7457, 19.7599, 19.7459, 19.7458, 19.7593, 19.7554, 19.7665,
        16.2255, 16.2245, 16.2272, 16.2198, 16.2278, 16.2307, 16.2213, 16.2275,
        16.2169, 16.2175, 16.2238, 16.2110, 16.2194, 16.2199, 16.2173, 16.2258,
        19.8113, 19.8245, 19.8298, 19.8094, 19.8192, 19.8272, 19.8084, 19.8195,
        19.8195, 19.8125, 19.8064, 19.8008, 19.8160, 19.8079, 19.8023, 19.8244,
        19.5270, 19.5736, 19.5517, 19.5231, 19.5319, 19.5184, 19.5026, 19.5115,
        19.5442, 19.5426, 19.5463, 19.5654, 19.5684, 19.5255, 19.5413, 19.5314,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
        21.2379, 21.2361, 21.2470, 21.2142, 21.2199, 21.2323, 21.1694, 21.1604,
        21.1668, 21.1711, 21.2003, 21.1549, 21.1991, 21.2050, 21.1581, 21.2240,
        23.7894, 23.8776, 23.8179, 23.7686, 23.8318, 23.7886, 23.8019, 23.8453,
        23.8122, 23.8448, 23.8015, 23.7803, 23.8397, 23.7724, 23.7819, 23.8562,
        15.1498, 15.1836, 15.1743, 15.1866, 15.1740, 15.1821, 15.1893, 15.1701,
        15.1464, 15.1976, 15.1700, 15.1676, 15.1898, 15.1758, 15.2085, 15.2382,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])
std_reconstruction_per_patch: tensor([26.9938, 27.0290, 27.0266, 27.0009, 27.0090, 26.9919, 27.0067, 26.9976,
        27.0009, 27.0104, 27.0225, 27.0193, 27.0111, 26.9966, 26.9964, 26.9839,
        28.0171, 28.0094, 28.0022, 28.0172, 28.0114, 27.9783, 27.9820, 27.9940,
        28.0081, 27.9963, 28.0012, 28.0153, 28.0233, 28.0005, 28.0042, 28.0200,
        28.7452, 28.7560, 28.7502, 28.7484, 28.7603, 28.7386, 28.7562, 28.7597,
        28.7456, 28.7348, 28.7395, 28.7480, 28.7356, 28.7371, 28.7505, 28.7557,
        17.5249, 17.5373, 17.5418, 17.5371, 17.5383, 17.5264, 17.5477, 17.5309,
        17.5185, 17.5167, 17.5299, 17.5131, 17.5093, 17.5224, 17.5211, 17.5265,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
        10.0087, 10.0059, 10.0142, 10.0244, 10.0108, 10.0212, 10.0230, 10.0130,
        10.0089, 10.0093, 10.0085, 10.0182, 10.0069, 10.0147, 10.0167, 10.0169,
        19.7662, 19.7463, 19.7408, 19.7463, 19.7457, 19.7583, 19.7552, 19.7618,
        19.7600, 19.7457, 19.7599, 19.7459, 19.7458, 19.7593, 19.7554, 19.7665,
        16.2255, 16.2245, 16.2272, 16.2198, 16.2278, 16.2307, 16.2213, 16.2275,
        16.2169, 16.2175, 16.2238, 16.2110, 16.2194, 16.2199, 16.2173, 16.2258,
        19.8113, 19.8245, 19.8298, 19.8094, 19.8192, 19.8272, 19.8084, 19.8195,
        19.8195, 19.8125, 19.8064, 19.8008, 19.8160, 19.8079, 19.8023, 19.8244,
        19.5270, 19.5736, 19.5517, 19.5231, 19.5319, 19.5184, 19.5026, 19.5115,
        19.5442, 19.5426, 19.5463, 19.5654, 19.5684, 19.5255, 19.5413, 19.5314,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
        21.2379, 21.2361, 21.2470, 21.2142, 21.2199, 21.2323, 21.1694, 21.1604,
        21.1668, 21.1711, 21.2003, 21.1549, 21.1991, 21.2050, 21.1581, 21.2240,
        23.7894, 23.8776, 23.8179, 23.7686, 23.8318, 23.7886, 23.8019, 23.8453,
        23.8122, 23.8448, 23.8015, 23.7803, 23.8397, 23.7724, 23.7819, 23.8562,
        15.1498, 15.1836, 15.1743, 15.1866, 15.1740, 15.1821, 15.1893, 15.1701,
        15.1464, 15.1976, 15.1700, 15.1676, 15.1898, 15.1758, 15.2085, 15.2382,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])
03/04/2024 09:30:02 - INFO - __main__ - torch.Size([3, 256, 256])
03/04/2024 09:30:02 - INFO - __main__ - torch.Size([3, 256, 256])
wandb: Waiting for W&B process to finish... (success).
wandb: - 1.755 MB of 1.756 MB uploaded (0.000 MB deduped)wandb: \ 1.755 MB of 1.756 MB uploaded (0.000 MB deduped)wandb: | 1.755 MB of 1.756 MB uploaded (0.000 MB deduped)wandb: / 1.755 MB of 1.756 MB uploaded (0.000 MB deduped)wandb: - 1.755 MB of 1.756 MB uploaded (0.000 MB deduped)wandb: \ 1.755 MB of 1.756 MB uploaded (0.000 MB deduped)wandb: | 1.755 MB of 1.756 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run history:
wandb:      mean_std_value ‚ñÅ
wandb: mean_variance_value ‚ñÅ
wandb: 
wandb: Run summary:
wandb:      mean_std_value 0.106
wandb: mean_variance_value 0.022
wandb: 
wandb: üöÄ View run pious-feather-198 at: https://wandb.ai/stefania_radu/pixel-semantic-scripts_visualization/runs/rqc1n7p3
wandb: Ô∏è‚ö° View job at https://wandb.ai/stefania_radu/pixel-semantic-scripts_visualization/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExOTExMzkwMw==/version_details/v48
wandb: Synced 6 W&B file(s), 15 media file(s), 4 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240304_092830-rqc1n7p3/logs

###############################################################################
H√°br√≥k Cluster
Job 7485428 for user s3919609
Finished at: Mon Mar  4 09:30:12 CET 2024

Job details:
============

Job ID              : 7485428
Name                : experiments_pixel_uncertainty_m0.7
User                : s3919609
Partition           : regularshort
Nodes               : node103
Number of Nodes     : 1
Cores               : 1
Number of Tasks     : 1
State               : COMPLETED
Submit              : 2024-03-04T09:27:54
Start               : 2024-03-04T09:28:13
End                 : 2024-03-04T09:30:12
Reserved walltime   : 01:00:00
Used walltime       : 00:01:59
Used CPU time       : 00:01:41 (efficiency: 85.55%)
% User (Computation): 91.74%
% System (I/O)      :  8.26%
Mem reserved        : 10G
Max Mem (Node/step) : 1.28G (node103, per node)
Full Max Mem usage  : 1.28G
Total Disk Read     : 582.97M
Total Disk Write    : 985.83K

Acknowledgements:
=================

Please see this page for information about acknowledging H√°br√≥k in your publications:

https://wiki.hpc.rug.nl/habrok/introduction/scientific_output

################################################################################
