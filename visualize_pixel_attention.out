The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) 2023.01   2) StdEnv
wandb: Currently logged in as: stefania_radu. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /home2/s3919609/pixel-semantic/wandb/run-20240220_173009-2jzzsmvp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vermilion-fireworks-160
wandb: ‚≠êÔ∏è View project at https://wandb.ai/stefania_radu/pixel-semantic-scripts_visualization
wandb: üöÄ View run at https://wandb.ai/stefania_radu/pixel-semantic-scripts_visualization/runs/2jzzsmvp
02/20/2024 17:30:13 - INFO - pixel.data.rendering.rendering_utils - loading text renderer configuration file https://huggingface.co/Team-PIXEL/pixel-base/resolve/main/text_renderer_config.json from cache at /home2/s3919609/.cache/huggingface/transformers/892d6a02d7c441000de399de59ed70d943a81f7b0f536523b4af1111677a8508.e332b34c9c05756dd4aa51d8fa33461dbd79604752296d185f03f8004db30700
02/20/2024 17:30:13 - INFO - pixel.data.rendering.rendering_utils - loading font file https://huggingface.co/Team-PIXEL/pixel-base/resolve/main/GoNotoCurrent.ttf from cache at /home2/s3919609/.cache/huggingface/transformers/49e6dc219d1a1a1c9236acaf05a48b542002016a6dc877ee72baab085a84257b.3f28e7f4b38e1efe1b6da4a3732404c19d4c6a614ff32dce90a251e293d4ce58
02/20/2024 17:30:13 - INFO - pixel.data.rendering.pygame_renderer - Loading font from /home2/s3919609/.cache/huggingface/transformers/49e6dc219d1a1a1c9236acaf05a48b542002016a6dc877ee72baab085a84257b.3f28e7f4b38e1efe1b6da4a3732404c19d4c6a614ff32dce90a251e293d4ce58
02/20/2024 17:30:13 - INFO - pixel.data.rendering.rendering_utils - Text renderer PyGameTextRenderer {
  "background_color": "white",
  "dpi": 120,
  "font_color": "black",
  "font_file": "49e6dc219d1a1a1c9236acaf05a48b542002016a6dc877ee72baab085a84257b.3f28e7f4b38e1efe1b6da4a3732404c19d4c6a614ff32dce90a251e293d4ce58",
  "font_size": 8,
  "max_seq_length": 256,
  "pad_size": 3,
  "pixels_per_patch": 16,
  "text_renderer_type": "PyGameTextRenderer"
}

02/20/2024 17:30:15 - INFO - pixel.utils.modeling - Truncating position embeddings to 256
02/20/2024 17:30:15 - INFO - pixel.utils.modeling - Truncating decoder position embeddings to 256
02/20/2024 17:30:15 - INFO - __main__ - Running PIXEL masked autoencoding with pixel reconstruction
02/20/2024 17:30:15 - INFO - __main__ - Applying span masking with "max_span_length = 6" , "cumulative_span_weights = [0.2, 0.4, 0.6, 0.8, 0.9, 1.0]"  and "spacing = span"
02/20/2024 17:30:15 - INFO - __main__ - Masked count: 90, ratio = 0.3516
02/20/2024 17:30:15 - INFO - __main__ - Monte Carlos samples: 100
02/20/2024 17:30:15 - INFO - __main__ - Training mode: True
all_attention (samples, layers, batch_size, num_heads, sequence_length, sequence_length): torch.Size([100, 12, 12, 167, 167])
all_attention after mean: torch.Size([12, 12, 167, 167])
02/20/2024 17:31:28 - INFO - __main__ - std_predictions shape: torch.Size([3, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 167, 167])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 167, 167])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 167, 167])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 167, 167])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 167, 167])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 167, 167])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 167, 167])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 167, 167])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 167, 167])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 167, 167])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 167, 167])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
mask.shape: torch.Size([3, 256, 256])
attentions.shape: torch.Size([12, 167, 167])
all_heads_attentions.shape: torch.Size([12, 256, 256])
all_heads_attentions_image: torch.Size([12, 256, 256])
all_layers_attentions: torch.Size([12, 12, 256, 256])
attention_grid: torch.Size([3, 3098, 3098])
attention_grid 0 : tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.9491,  ..., 1.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])
attention_grid 1 : tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.9491,  ..., 1.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])
Are the channels different? False
02/20/2024 17:31:32 - INFO - __main__ - Mean variance for whole image: 0.017
02/20/2024 17:31:32 - INFO - __main__ - Mean std for whole image: 0.101
02/20/2024 17:31:32 - INFO - __main__ - mean_std shape: (3, 256, 256)
02/20/2024 17:31:32 - INFO - __main__ - std_predictions shape: torch.Size([3, 256, 256])
Traceback (most recent call last):
  File "/home2/s3919609/pixel-semantic/scripts/visualization/visualize_pixel_uncertainty.py", line 561, in <module>
    main(parsed_args)
  File "/home2/s3919609/pixel-semantic/scripts/visualization/visualize_pixel_uncertainty.py", line 511, in main
    experiments_table.add_data(args.input_str, num_samples, mean_std_value, mean_variance_value, std_predictions.shape[1], mask_ratio, masked_count, args.masking_max_span_length, args.masking_cumulative_span_weights)
NameError: name 'experiments_table' is not defined
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:      mean_std_value ‚ñÅ
wandb: mean_variance_value ‚ñÅ
wandb: 
wandb: Run summary:
wandb:      mean_std_value 0.101
wandb: mean_variance_value 0.017
wandb: 
wandb: üöÄ View run vermilion-fireworks-160 at: https://wandb.ai/stefania_radu/pixel-semantic-scripts_visualization/runs/2jzzsmvp
wandb: Ô∏è‚ö° View job at https://wandb.ai/stefania_radu/pixel-semantic-scripts_visualization/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExOTExMzkwMw==/version_details/v34
wandb: Synced 6 W&B file(s), 13 media file(s), 3 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240220_173009-2jzzsmvp/logs

###############################################################################
H√°br√≥k Cluster
Job 7269269 for user s3919609
Finished at: Tue Feb 20 17:31:43 CET 2024

Job details:
============

Job ID              : 7269269
Name                : visualize_pixel_attention
User                : s3919609
Partition           : regularshort
Nodes               : node104
Number of Nodes     : 1
Cores               : 1
Number of Tasks     : 1
State               : FAILED
Submit              : 2024-02-20T17:29:57
Start               : 2024-02-20T17:29:58
End                 : 2024-02-20T17:31:43
Reserved walltime   : 01:00:00
Used walltime       : 00:01:45
Used CPU time       : 00:01:28 (efficiency: 84.61%)
% User (Computation): 94.95%
% System (I/O)      :  5.05%
Mem reserved        : 10G
Max Mem (Node/step) : 2.95G (node104, per node)
Full Max Mem usage  : 2.95G
Total Disk Read     : 684.37M
Total Disk Write    : 1016.17K

Acknowledgements:
=================

Please see this page for information about acknowledging H√°br√≥k in your publications:

https://wiki.hpc.rug.nl/habrok/introduction/scientific_output

################################################################################
