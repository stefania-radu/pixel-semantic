The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) 2023.01   2) StdEnv
language: Amharic
{'pixel_values': <PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71DDA790>, 'num_patches': 201}
<PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71DDA790>
shape: img_array = np.array(sample['pixel_values']) (16, 8464)
shape: image = transforms(Image.fromarray(img_array)).unsqueeze(0) torch.Size([1, 3, 16, 8464])
shape: original_img = model.unpatchify(model.patchify(image)).squeeze() torch.Size([3, 368, 368])
language: Arabic
Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]Resolving data files:  22%|██▏       | 11/50 [00:00<00:00, 103.11it/s]Resolving data files: 100%|██████████| 50/50 [00:00<00:00, 306.87it/s]
Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]Resolving data files:  44%|████▍     | 22/50 [00:00<00:00, 147.26it/s]Resolving data files: 100%|██████████| 50/50 [00:00<00:00, 333.85it/s]
{'pixel_values': <PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71D70490>, 'num_patches': 465}
<PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71D70490>
shape: img_array = np.array(sample['pixel_values']) (16, 8464)
shape: image = transforms(Image.fromarray(img_array)).unsqueeze(0) torch.Size([1, 3, 16, 8464])
shape: original_img = model.unpatchify(model.patchify(image)).squeeze() torch.Size([3, 368, 368])
language: Finnish
Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files:  61%|██████    | 14/23 [00:00<00:00, 70.70it/s]Resolving data files: 100%|██████████| 23/23 [00:00<00:00, 115.96it/s]
Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 23/23 [00:00<00:00, 60940.61it/s]
{'pixel_values': <PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71D1DC40>, 'num_patches': 284}
<PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71D1DC40>
shape: img_array = np.array(sample['pixel_values']) (16, 8464)
shape: image = transforms(Image.fromarray(img_array)).unsqueeze(0) torch.Size([1, 3, 16, 8464])
shape: original_img = model.unpatchify(model.patchify(image)).squeeze() torch.Size([3, 368, 368])
language: Hausa
{'pixel_values': <PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71CD7EB0>, 'num_patches': 464}
<PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71CD7EB0>
shape: img_array = np.array(sample['pixel_values']) (16, 8464)
shape: image = transforms(Image.fromarray(img_array)).unsqueeze(0) torch.Size([1, 3, 16, 8464])
shape: original_img = model.unpatchify(model.patchify(image)).squeeze() torch.Size([3, 368, 368])
language: Indonesian
Resolving data files:   0%|          | 0/24 [00:00<?, ?it/s]Resolving data files:   4%|▍         | 1/24 [00:00<00:05,  4.48it/s]Resolving data files:  50%|█████     | 12/24 [00:00<00:00, 29.06it/s]Resolving data files:  88%|████████▊ | 21/24 [00:00<00:00, 36.51it/s]Resolving data files: 100%|██████████| 24/24 [00:00<00:00, 36.42it/s]
Resolving data files:   0%|          | 0/24 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 24/24 [00:00<00:00, 49932.19it/s]
{'pixel_values': <PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71DAD6D0>, 'num_patches': 386}
<PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71DAD6D0>
shape: img_array = np.array(sample['pixel_values']) (16, 8464)
shape: image = transforms(Image.fromarray(img_array)).unsqueeze(0) torch.Size([1, 3, 16, 8464])
shape: original_img = model.unpatchify(model.patchify(image)).squeeze() torch.Size([3, 368, 368])
language: Igbo
{'pixel_values': <PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71C909A0>, 'num_patches': 300}
<PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71C909A0>
shape: img_array = np.array(sample['pixel_values']) (16, 8464)
shape: image = transforms(Image.fromarray(img_array)).unsqueeze(0) torch.Size([1, 3, 16, 8464])
shape: original_img = model.unpatchify(model.patchify(image)).squeeze() torch.Size([3, 368, 368])
language: Korean
Resolving data files:   0%|          | 0/39 [00:00<?, ?it/s]Resolving data files:  10%|█         | 4/39 [00:00<00:01, 31.00it/s]Resolving data files: 100%|██████████| 39/39 [00:00<00:00, 217.13it/s]
Resolving data files:   0%|          | 0/39 [00:00<?, ?it/s]Resolving data files: 100%|██████████| 39/39 [00:00<00:00, 82698.61it/s]
{'pixel_values': <PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71CE3C40>, 'num_patches': 522}
<PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71CE3C40>
shape: img_array = np.array(sample['pixel_values']) (16, 8464)
shape: image = transforms(Image.fromarray(img_array)).unsqueeze(0) torch.Size([1, 3, 16, 8464])
shape: original_img = model.unpatchify(model.patchify(image)).squeeze() torch.Size([3, 368, 368])
language: Luganda
{'pixel_values': <PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71C63940>, 'num_patches': 159}
<PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71C63940>
shape: img_array = np.array(sample['pixel_values']) (16, 8464)
shape: image = transforms(Image.fromarray(img_array)).unsqueeze(0) torch.Size([1, 3, 16, 8464])
shape: original_img = model.unpatchify(model.patchify(image)).squeeze() torch.Size([3, 368, 368])
language: Nigerian Pidgin
{'pixel_values': <PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71C6AB50>, 'num_patches': 138}
<PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71C6AB50>
shape: img_array = np.array(sample['pixel_values']) (16, 8464)
shape: image = transforms(Image.fromarray(img_array)).unsqueeze(0) torch.Size([1, 3, 16, 8464])
shape: original_img = model.unpatchify(model.patchify(image)).squeeze() torch.Size([3, 368, 368])
language: Kinyarwanda
{'pixel_values': <PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71CD7B20>, 'num_patches': 523}
<PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71CD7B20>
shape: img_array = np.array(sample['pixel_values']) (16, 8464)
shape: image = transforms(Image.fromarray(img_array)).unsqueeze(0) torch.Size([1, 3, 16, 8464])
shape: original_img = model.unpatchify(model.patchify(image)).squeeze() torch.Size([3, 368, 368])
language: Swahili
{'pixel_values': <PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71CD7FD0>, 'num_patches': 289}
<PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71CD7FD0>
shape: img_array = np.array(sample['pixel_values']) (16, 8464)
shape: image = transforms(Image.fromarray(img_array)).unsqueeze(0) torch.Size([1, 3, 16, 8464])
shape: original_img = model.unpatchify(model.patchify(image)).squeeze() torch.Size([3, 368, 368])
language: Telugu
{'pixel_values': <PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71CEDCD0>, 'num_patches': 299}
<PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71CEDCD0>
shape: img_array = np.array(sample['pixel_values']) (16, 8464)
shape: image = transforms(Image.fromarray(img_array)).unsqueeze(0) torch.Size([1, 3, 16, 8464])
shape: original_img = model.unpatchify(model.patchify(image)).squeeze() torch.Size([3, 368, 368])
language: Wolof
{'pixel_values': <PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71CEDCD0>, 'num_patches': 208}
<PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71CEDCD0>
shape: img_array = np.array(sample['pixel_values']) (16, 8464)
shape: image = transforms(Image.fromarray(img_array)).unsqueeze(0) torch.Size([1, 3, 16, 8464])
shape: original_img = model.unpatchify(model.patchify(image)).squeeze() torch.Size([3, 368, 368])
language: Yorùbá
{'pixel_values': <PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71C49670>, 'num_patches': 27}
<PIL.PngImagePlugin.PngImageFile image mode=L size=8464x16 at 0x7F9B71C49670>
shape: img_array = np.array(sample['pixel_values']) (16, 8464)
shape: image = transforms(Image.fromarray(img_array)).unsqueeze(0) torch.Size([1, 3, 16, 8464])
shape: original_img = model.unpatchify(model.patchify(image)).squeeze() torch.Size([3, 368, 368])

###############################################################################
Hábrók Cluster
Job 7123606 for user s3919609
Finished at: Tue Feb  6 09:05:28 CET 2024

Job details:
============

Job ID              : 7123606
Name                : visualize_renderer
User                : s3919609
Partition           : regularshort
Nodes               : node95
Number of Nodes     : 1
Cores               : 1
Number of Tasks     : 1
State               : COMPLETED
Submit              : 2024-02-06T08:57:21
Start               : 2024-02-06T08:57:22
End                 : 2024-02-06T09:05:27
Reserved walltime   : 01:00:00
Used walltime       : 00:08:05
Used CPU time       : 00:01:32 (efficiency: 19.12%)
% User (Computation): 79.29%
% System (I/O)      : 20.71%
Mem reserved        : 10G
Max Mem (Node/step) : 1.48G (node95, per node)
Full Max Mem usage  : 1.48G
Total Disk Read     : 14.51G
Total Disk Write    : 5.74M

Acknowledgements:
=================

Please see this page for information about acknowledging Hábrók in your publications:

https://wiki.hpc.rug.nl/habrok/introduction/scientific_output

################################################################################
