The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) 2023.01   2) StdEnv
11/24/2023 14:34:58 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False
11/24/2023 14:34:58 - INFO - __main__ - Training/evaluation parameters PIXELTrainingArguments(
_n_gpu=0,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=True,
do_train=True,
early_stopping=True,
early_stopping_patience=5,
eval_accumulation_steps=None,
eval_steps=500,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
greater_is_better=True,
group_by_length=False,
half_precision_backend=apex,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
log_predictions=True,
logging_dir=sst2-pixel-base-mean-pangocairo-256-64-4-3e-5-15000-42/runs/Nov24_14-34-58_a100gpu3,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=100,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=15000,
metric_for_best_model=eval_accuracy,
mp_parameters=,
no_cuda=False,
num_train_epochs=10.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=sst2-pixel-base-mean-pangocairo-256-64-4-3e-5-15000-42,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=64,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=False,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=sst2-pixel-base-mean-pangocairo-256-64-4-3e-5-15000-42,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=5,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=100,
weight_decay=0.0,
xpu_backend=None,
)
Loading Dataset Infos from /home2/s3919609/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
11/24/2023 14:34:59 - INFO - datasets.info - Loading Dataset Infos from /home2/s3919609/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
Overwrite dataset info from restored data version if exists.
11/24/2023 14:34:59 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home2/s3919609/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
11/24/2023 14:34:59 - INFO - datasets.info - Loading Dataset info from /home2/s3919609/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
Found cached dataset glue (/home2/s3919609/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
11/24/2023 14:35:00 - INFO - datasets.builder - Found cached dataset glue (/home2/s3919609/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Loading Dataset info from /home2/s3919609/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
11/24/2023 14:35:00 - INFO - datasets.info - Loading Dataset info from /home2/s3919609/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
[INFO|configuration_utils.py:648] 2023-11-24 14:35:00,218 >> loading configuration file https://huggingface.co/Team-PIXEL/pixel-base/resolve/main/config.json from cache at /home2/s3919609/.cache/huggingface/transformers/59ff3d0d96dd81437a11bdeafa5fad304064b3b0aea12a07ee093116a26dea72.73d98f997dc77ccebbd7d8f1f6b14deb97f467b7ecb1fd28b115bee90138c954
[INFO|configuration_utils.py:684] 2023-11-24 14:35:00,222 >> Model config PIXELConfig {
  "_name_or_path": "Team-PIXEL/pixel-base",
  "architectures": [
    "PIXELForPreTraining"
  ],
  "attention_probs_dropout_prob": 0.1,
  "decoder_hidden_size": 512,
  "decoder_intermediate_size": 2048,
  "decoder_num_attention_heads": 16,
  "decoder_num_hidden_layers": 8,
  "finetuning_task": "sst2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "image_size": [
    16,
    8464
  ],
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "mask_ratio": 0.25,
  "model_type": "pixel",
  "norm_pix_loss": true,
  "num_attention_heads": 12,
  "num_channels": 3,
  "num_hidden_layers": 12,
  "patch_size": 16,
  "qkv_bias": true,
  "torch_dtype": "float32",
  "transformers_version": "4.17.0"
}

11/24/2023 14:35:00 - INFO - __main__ - Using dropout with probability 0.1
[INFO|modeling_utils.py:1431] 2023-11-24 14:35:00,675 >> loading weights file https://huggingface.co/Team-PIXEL/pixel-base/resolve/main/pytorch_model.bin from cache at /home2/s3919609/.cache/huggingface/transformers/23b2e1587613b3d2d70a201b2326daa71157ae7590931531a595a6e714d79082.ac3a74956154d5b2812a6f968fa8dbd30b54b3f394c4c468d029f1ff79367617
[WARNING|modeling_utils.py:1693] 2023-11-24 14:35:02,728 >> Some weights of the model checkpoint at Team-PIXEL/pixel-base were not used when initializing PIXELForSequenceClassification: ['decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.mask_token', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.3.output.dense.bias']
- This IS expected if you are initializing PIXELForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PIXELForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1704] 2023-11-24 14:35:02,728 >> Some weights of PIXELForSequenceClassification were not initialized from the model checkpoint at Team-PIXEL/pixel-base and are newly initialized: ['pooler.ln.weight', 'classifier.bias', 'pooler.linear.weight', 'pooler.linear.bias', 'classifier.weight', 'pooler.ln.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
11/24/2023 14:35:02 - INFO - pixel.data.rendering.rendering_utils - loading text renderer configuration file https://huggingface.co/Team-PIXEL/pixel-base/resolve/main/text_renderer_config.json from cache at /home2/s3919609/.cache/huggingface/transformers/892d6a02d7c441000de399de59ed70d943a81f7b0f536523b4af1111677a8508.e332b34c9c05756dd4aa51d8fa33461dbd79604752296d185f03f8004db30700
11/24/2023 14:35:03 - INFO - pixel.data.rendering.rendering_utils - loading font file https://huggingface.co/Team-PIXEL/pixel-base/resolve/main/GoNotoCurrent.ttf from cache at /home2/s3919609/.cache/huggingface/transformers/49e6dc219d1a1a1c9236acaf05a48b542002016a6dc877ee72baab085a84257b.3f28e7f4b38e1efe1b6da4a3732404c19d4c6a614ff32dce90a251e293d4ce58
11/24/2023 14:35:03 - INFO - pixel.data.rendering.pangocairo_renderer - Loading font from /home2/s3919609/.cache/huggingface/transformers/49e6dc219d1a1a1c9236acaf05a48b542002016a6dc877ee72baab085a84257b.3f28e7f4b38e1efe1b6da4a3732404c19d4c6a614ff32dce90a251e293d4ce58
11/24/2023 14:35:04 - INFO - pixel.data.rendering.rendering_utils - Text renderer PangoCairoTextRenderer {
  "background_color": "white",
  "dpi": 120,
  "font_color": "black",
  "font_file": "49e6dc219d1a1a1c9236acaf05a48b542002016a6dc877ee72baab085a84257b.3f28e7f4b38e1efe1b6da4a3732404c19d4c6a614ff32dce90a251e293d4ce58",
  "font_size": 8,
  "fonts_list": [
    "C059",
    "Cantarell",
    "D050000L",
    "DejaVu Sans",
    "DejaVu Sans Mono",
    "Droid Sans",
    "Droid Sans Arabic",
    "Droid Sans Armenian",
    "Droid Sans Devanagari",
    "Droid Sans Ethiopic",
    "Droid Sans Fallback",
    "Droid Sans Georgian",
    "Droid Sans Hebrew",
    "Droid Sans Japanese",
    "Droid Sans Tamil",
    "Droid Sans Thai",
    "Go Noto Current",
    "Inconsolata",
    "MathJax_AMS",
    "MathJax_Caligraphic",
    "MathJax_Fraktur",
    "MathJax_Main",
    "MathJax_Math",
    "MathJax_SansSerif",
    "MathJax_Script",
    "MathJax_Size1",
    "MathJax_Size2",
    "MathJax_Size3",
    "MathJax_Size4",
    "MathJax_Typewriter",
    "MathJax_Vector",
    "MathJax_Vector-Bold",
    "MathJax_WinChrome",
    "MathJax_WinIE6",
    "Monospace",
    "Nimbus Mono PS",
    "Nimbus Roman",
    "Nimbus Sans",
    "Nimbus Sans Narrow",
    "P052",
    "STIX MathJax Alphabets",
    "STIX MathJax Arrows",
    "STIX MathJax DoubleStruck",
    "STIX MathJax Fraktur",
    "STIX MathJax Latin",
    "STIX MathJax Main",
    "STIX MathJax Marks",
    "STIX MathJax Misc",
    "STIX MathJax Monospace",
    "STIX MathJax Normal",
    "STIX MathJax Operators",
    "STIX MathJax SansSerif",
    "STIX MathJax Script",
    "STIX MathJax Shapes",
    "STIX MathJax Size1",
    "STIX MathJax Size2",
    "STIX MathJax Size3",
    "STIX MathJax Size4",
    "STIX MathJax Size5",
    "STIX MathJax Symbols",
    "STIX MathJax Variants",
    "Sans",
    "Serif",
    "Source Code Pro",
    "System-ui",
    "URW Bookman",
    "URW Gothic",
    "Ubuntu",
    "Ubuntu Condensed",
    "Ubuntu Mono",
    "Z003"
  ],
  "max_seq_length": 529,
  "pad_size": 3,
  "pixels_per_patch": 16,
  "rgb": false,
  "text_renderer_type": "PangoCairoTextRenderer"
}

11/24/2023 14:35:04 - INFO - pixel.utils.modeling - Truncating position embeddings to 256
11/24/2023 14:35:04 - INFO - __main__ - Sample 14592 of the training set: {'sentence': 'a great movie ', 'label': 1, 'idx': 14592, 'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]],

        [[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]],

        [[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])}.
11/24/2023 14:35:04 - INFO - __main__ - Sample 3278 of the training set: {'sentence': 'entertaining , if somewhat standardized , action ', 'label': 1, 'idx': 3278, 'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]],

        [[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]],

        [[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])}.
11/24/2023 14:35:04 - INFO - __main__ - Sample 36048 of the training set: {'sentence': 'even when there are lulls , the emotions seem authentic , ', 'label': 1, 'idx': 36048, 'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]],

        [[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]],

        [[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])}.
11/24/2023 14:35:04 - INFO - __main__ - Sample 250 of the eval set: {'sentence': "one of the more intelligent children 's movies to hit theaters this year . ", 'label': 1, 'idx': 250, 'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]],

        [[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]],

        [[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])}.
11/24/2023 14:35:04 - INFO - __main__ - Sample 228 of the eval set: {'sentence': 'it provides an honest look at a community striving to anchor itself in new grounds . ', 'label': 1, 'idx': 228, 'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]],

        [[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]],

        [[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])}.
11/24/2023 14:35:04 - INFO - __main__ - Sample 142 of the eval set: {'sentence': "what better message than ` love thyself ' could young women of any size receive ? ", 'label': 1, 'idx': 142, 'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]],

        [[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]],

        [[1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         ...,
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.],
         [1., 1., 1.,  ..., 1., 1., 1.]]]), 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0.])}.
/home2/s3919609/pixel-semantic/scripts/training/run_glue.py:604: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("glue", data_args.task_name)
[INFO|trainer.py:430] 2023-11-24 14:35:05,602 >> max_steps is given, it will override any value given in num_train_epochs
/scratch/s3919609/conda/envs/pixel-sem-env/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1279] 2023-11-24 14:35:05,622 >> ***** Running training *****
[INFO|trainer.py:1280] 2023-11-24 14:35:05,622 >>   Num examples = 67349
[INFO|trainer.py:1281] 2023-11-24 14:35:05,622 >>   Num Epochs = 58
[INFO|trainer.py:1282] 2023-11-24 14:35:05,622 >>   Instantaneous batch size per device = 64
[INFO|trainer.py:1283] 2023-11-24 14:35:05,622 >>   Total train batch size (w. parallel, distributed & accumulation) = 256
[INFO|trainer.py:1284] 2023-11-24 14:35:05,622 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1285] 2023-11-24 14:35:05,622 >>   Total optimization steps = 15000
[INFO|integrations.py:575] 2023-11-24 14:35:05,626 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: stefania_radu. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /home2/s3919609/pixel-semantic/wandb/run-20231124_143509-3mghpdft
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sst2-pixel-base-mean-pangocairo-256-64-4-3e-5-15000-42
wandb: ‚≠êÔ∏è View project at https://wandb.ai/stefania_radu/pixel-experiments
wandb: üöÄ View run at https://wandb.ai/stefania_radu/pixel-experiments/runs/3mghpdft
  0%|          | 0/15000 [00:00<?, ?it/s]  0%|          | 1/15000 [00:18<77:04:32, 18.50s/it]  0%|          | 2/15000 [00:32<66:58:26, 16.08s/it]  0%|          | 3/15000 [00:46<62:39:44, 15.04s/it]  0%|          | 4/15000 [01:00<60:38:22, 14.56s/it]  0%|          | 5/15000 [01:14<59:07:31, 14.19s/it]  0%|          | 6/15000 [01:28<58:46:05, 14.11s/it]  0%|          | 7/15000 [01:41<58:32:07, 14.06s/it]  0%|          | 8/15000 [01:55<58:26:41, 14.03s/it]  0%|          | 9/15000 [02:09<58:06:09, 13.95s/it]  0%|          | 10/15000 [02:23<58:05:43, 13.95s/it]  0%|          | 11/15000 [02:37<57:59:52, 13.93s/it]  0%|          | 12/15000 [02:51<58:02:05, 13.94s/it]  0%|          | 13/15000 [03:05<58:03:00, 13.94s/it]  0%|          | 14/15000 [03:19<57:55:19, 13.91s/it]  0%|          | 15/15000 [03:33<58:27:45, 14.05s/it]  0%|          | 16/15000 [03:47<58:37:18, 14.08s/it]  0%|          | 17/15000 [04:01<58:40:16, 14.10s/it]  0%|          | 18/15000 [04:16<58:40:26, 14.10s/it]  0%|          | 19/15000 [04:30<58:46:20, 14.12s/it]  0%|          | 20/15000 [04:44<58:46:09, 14.12s/it]  0%|          | 21/15000 [04:58<58:32:21, 14.07s/it]  0%|          | 22/15000 [05:12<59:01:52, 14.19s/it]  0%|          | 23/15000 [05:26<58:47:56, 14.13s/it]  0%|          | 24/15000 [05:41<59:22:11, 14.27s/it]  0%|          | 25/15000 [05:56<60:16:29, 14.49s/it]  0%|          | 26/15000 [06:10<59:46:31, 14.37s/it]  0%|          | 27/15000 [06:24<59:09:26, 14.22s/it]  0%|          | 28/15000 [06:38<59:12:30, 14.24s/it]  0%|          | 29/15000 [06:52<58:44:26, 14.13s/it]  0%|          | 30/15000 [07:06<58:32:20, 14.08s/it]  0%|          | 31/15000 [07:20<58:10:29, 13.99s/it]  0%|          | 32/15000 [07:34<58:15:43, 14.01s/it]  0%|          | 33/15000 [07:48<58:02:45, 13.96s/it]  0%|          | 34/15000 [08:02<57:58:07, 13.94s/it]  0%|          | 35/15000 [08:16<58:00:55, 13.96s/it]  0%|          | 36/15000 [08:29<57:53:50, 13.93s/it]  0%|          | 37/15000 [08:43<58:00:06, 13.95s/it]  0%|          | 38/15000 [08:57<58:04:38, 13.97s/it]slurmstepd: error: *** JOB 6351654 ON a100gpu3 CANCELLED AT 2023-11-24T14:44:16 DUE TO TIME LIMIT ***

###############################################################################
H√°br√≥k Cluster
Job 6351654 for user s3919609
Finished at: Fri Nov 24 14:44:18 CET 2023

Job details:
============

Job ID              : 6351654
Name                : pixel_test_gpu
User                : s3919609
Partition           : gpushort
Nodes               : a100gpu3
Number of Nodes     : 1
Cores               : 40
Number of Tasks     : 1
State               : TIMEOUT,CANCELLED
Submit              : 2023-11-24T14:33:49
Start               : 2023-11-24T14:33:49
End                 : 2023-11-24T14:44:17
Reserved walltime   : 00:10:00
Used walltime       : 00:10:28
Used CPU time       : 05:57:30 (efficiency: 85.39%)
% User (Computation): 81.16%
% System (I/O)      : 18.84%
Mem reserved        : 32G
Max Mem (Node/step) : 18.61G (a100gpu3, per node)
Full Max Mem usage  : 18.61G
Total Disk Read     : 12.57M
Total Disk Write    : 364.32K

Acknowledgements:
=================

Please see this page for information about acknowledging H√°br√≥k in your publications:

https://wiki.hpc.rug.nl/habrok/introduction/scientific_output

################################################################################
